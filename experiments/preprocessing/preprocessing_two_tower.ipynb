{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320b2d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Add new system path to import config file\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../../\")))\n",
    "# Build path two levels up\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01e5757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run ../eda/read_data.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5fe98d",
   "metadata": {},
   "source": [
    "# --------------------------\n",
    "# 0. Train Test Split\n",
    "# --------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5090462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "train_X, val_X = train_test_split(train_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffee38b",
   "metadata": {},
   "source": [
    "# --------------------------\n",
    "# 1. User Tower Feature Engineering\n",
    "# --------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eeaac04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_user_features(users: pd.DataFrame, save_dir: str = \"user_encoders\") -> pd.DataFrame:\n",
    "    \"\"\"Encode categorical and time-based user features and save LabelEncoders.\"\"\"\n",
    "    user_features = users.copy()\n",
    "    \n",
    "    # Create directory to save encoders if not exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Define categorical columns to encode\n",
    "    categorical_user_cols = ['platform', 'os_version', 'model', 'networkType', 'district', 'language_selected']\n",
    "\n",
    "    # Encode categorical user features\n",
    "    for col in categorical_user_cols:\n",
    "        encoder_path = os.path.join(save_dir, f\"user_{col}_encoder.pkl\")\n",
    "        # Check if encoder exists\n",
    "        if os.path.isfile(encoder_path):\n",
    "            # Load existing encoder\n",
    "            le = joblib.load(encoder_path)\n",
    "        else:\n",
    "            # Fit new encoder and save\n",
    "            le = LabelEncoder()\n",
    "            le.fit(user_features[col].astype(str))\n",
    "            joblib.dump(le, encoder_path)\n",
    "        # Apply encoder to column\n",
    "        user_features[col] = le.transform(user_features[col].astype(str))\n",
    "\n",
    "    # Convert timestamps to UTC\n",
    "    user_features['last_active_at'] = pd.to_datetime(user_features['last_active_at'], utc=True, errors='coerce')\n",
    "    user_features['created_datetime'] = pd.to_datetime(user_features['created_datetime'], utc=True, errors='coerce')\n",
    "\n",
    "    # Current UTC time\n",
    "    now = pd.Timestamp.now(tz='UTC')\n",
    "\n",
    "    # Create recency features\n",
    "    user_features['days_since_last_active'] = (now - user_features['last_active_at']).dt.days\n",
    "    user_features['days_since_signup'] = (now - user_features['created_datetime']).dt.days\n",
    "\n",
    "    # Fill missing values with 0\n",
    "    return user_features.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15285337",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_users = preprocess_user_features(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2867b3",
   "metadata": {},
   "source": [
    "# --------------------------\n",
    "# 2. Content Tower Feature Engineering\n",
    "# --------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12b29a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def preprocess_content_features(train_df: pd.DataFrame, save_dir: str = \"content_encoders\", max_features: int = 5000, n_svd: int = 128):\n",
    "    \"\"\"Encode categorical and text features for content; save/reuse encoders, TF-IDF, and SVD.\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    categorical_content_cols = ['newsType', 'newsLanguage', 'sourceName', 'newsDistrict']\n",
    "    content_features = train_df.copy()\n",
    "\n",
    "    # Encode categorical columns\n",
    "    for col in categorical_content_cols:\n",
    "        encoder_path = os.path.join(save_dir, f\"content_{col}_encoder.pkl\")\n",
    "        if os.path.isfile(encoder_path):\n",
    "            le = joblib.load(encoder_path)\n",
    "        else:\n",
    "            le = LabelEncoder()\n",
    "            le.fit(content_features[col].astype(str))\n",
    "            joblib.dump(le, encoder_path)\n",
    "        content_features[col] = le.transform(content_features[col].astype(str))\n",
    "\n",
    "    # Prepare text: title*3 + content\n",
    "    content_features['text'] = content_features['title'].fillna('') * 3 + \" \" + content_features['content'].fillna('')\n",
    "\n",
    "    # TF-IDF Vectorizer\n",
    "    tfidf_path = os.path.join(save_dir, \"tfidf_vectorizer.pkl\")\n",
    "    if os.path.isfile(tfidf_path):\n",
    "        tfidf = joblib.load(tfidf_path)\n",
    "        text_features = tfidf.transform(content_features['text'])\n",
    "    else:\n",
    "        tfidf = TfidfVectorizer(max_features=max_features)\n",
    "        text_features = tfidf.fit_transform(content_features['text'])\n",
    "        joblib.dump(tfidf, tfidf_path)\n",
    "\n",
    "    # Truncated SVD for dimensionality reduction\n",
    "    svd_path = os.path.join(save_dir, \"svd_model.pkl\")\n",
    "    if os.path.isfile(svd_path):\n",
    "        svd = joblib.load(svd_path)\n",
    "        text_emb = svd.transform(text_features)\n",
    "    else:\n",
    "        svd = TruncatedSVD(n_components=n_svd, random_state=42)\n",
    "        text_emb = svd.fit_transform(text_features)\n",
    "        joblib.dump(svd, svd_path)\n",
    "\n",
    "    # Concatenate embeddings with original dataframe\n",
    "    text_emb_df = pd.DataFrame(text_emb, columns=[f'text_emb_{i}' for i in range(text_emb.shape[1])])\n",
    "    content_features = pd.concat([content_features.reset_index(drop=True), text_emb_df], axis=1)\n",
    "\n",
    "    return content_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b6e94c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = preprocess_content_features(train_X)\n",
    "val_X = preprocess_content_features(val_X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff97d3b",
   "metadata": {},
   "source": [
    "# --------------------------\n",
    "# 3. Interaction / Label Engineering\n",
    "# --------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efe64ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_training_data(events: pd.DataFrame, user_features: pd.DataFrame, content_features: pd.DataFrame):\n",
    "    \"\"\"Assign engagement scores and merge only selected content events with user/content features.\"\"\"\n",
    "    \n",
    "    selected_hashes = list(content_features['hashid'].unique())\n",
    "    \n",
    "    # Filter events to only include selected hashIds\n",
    "    events = events[events['hashId'].isin(selected_hashes)].copy()\n",
    "\n",
    "    # Assign engagement scores to event types\n",
    "    event_weights = {\n",
    "        'TimeSpent-Front': 0.3,\n",
    "        'TimeSpent-Back': 0.5,\n",
    "        'News Bookmarked': 1.0,\n",
    "        'News Shared': 1.0\n",
    "    }\n",
    "    events['engagement_score'] = events['event_type'].map(event_weights).fillna(0)\n",
    "\n",
    "    # Convert timestamp to UTC\n",
    "    events['eventTimestamp'] = pd.to_datetime(events['eventTimestamp'], unit='ms', utc=True)\n",
    "\n",
    "    # Merge with user features\n",
    "    train_df = events.merge(user_features, left_on='deviceId', right_on='deviceid', how='left', suffixes=('', '_user'))\n",
    "    # Merge with content features\n",
    "    train_df = train_df.merge(content_features, left_on='hashId', right_on='hashid', how='left', suffixes=('', '_content'))\n",
    "\n",
    "    # User and content tower columns\n",
    "    user_tower_cols = ['platform', 'os_version', 'model', 'networkType', 'district_user', 'language_selected',\n",
    "                       'days_since_last_active', 'days_since_signup']\n",
    "    content_tower_cols = ['newsType', 'newsLanguage', 'sourceName', 'newsDistrict'] + [f'text_emb_{i}' for i in range(128)]\n",
    "\n",
    "    # Target\n",
    "    target_col = 'engagement_score'\n",
    "\n",
    "    # Split features and target\n",
    "    X_user = train_df[user_tower_cols]\n",
    "    X_content = train_df[content_tower_cols]\n",
    "    y = train_df[target_col]\n",
    "\n",
    "    return X_user, X_content, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "433f90c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_user, tX_content, ty = create_training_data(events=events, user_features=processed_users, content_features=train_X)\n",
    "vX_user, vX_content, vy = create_training_data(events=events, user_features=processed_users, content_features=val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6400ad9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training and validation data using pickle\n",
    "\n",
    "data_to_save = {\n",
    "    \"tX_user\": tX_user,\n",
    "    \"tX_content\": tX_content,\n",
    "    \"ty\": ty,\n",
    "    \"vX_user\": vX_user,\n",
    "    \"vX_content\": vX_content,\n",
    "    \"vy\": vy\n",
    "}\n",
    "\n",
    "with open(\"train_val_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_to_save, f)\n",
    "\n",
    "print(\"✅ Training and validation data saved to train_val_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f969af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and validation data using pickle\n",
    "\n",
    "with open(\"train_val_data.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "tX_user = data[\"tX_user\"]\n",
    "tX_content = data[\"tX_content\"]\n",
    "ty = data[\"ty\"]\n",
    "vX_user = data[\"vX_user\"]\n",
    "vX_content = data[\"vX_content\"]\n",
    "vy = data[\"vy\"]\n",
    "\n",
    "print(\"✅ Data loaded successfully from pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
