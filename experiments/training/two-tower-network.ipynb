{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88784125",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49a934c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "# Add new system path to import MODEL_CONFIG file\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../../\")))\n",
    "# Build path two levels up\n",
    "MODEL_CONFIG = {\n",
    "    \"data_path\": \"artifacts/train_val_data.pkl\",\n",
    "    \"mlruns_dir\": \"artifacts/mlruns\",\n",
    "    \"experiment_name\": \"two_tower_recommender\",\n",
    "    \"num_trials\": 10,\n",
    "    \"num_epochs\": 10,\n",
    "    \"hpo_params\": {\n",
    "        \"embedding_dim\": [32, 64, 96],\n",
    "        \"dropout_range\": (0.1, 0.3),\n",
    "        \"lr_range\": (1e-4, 1e-2),\n",
    "        \"batch_size\": [1024, 2048]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e522f137",
   "metadata": {},
   "source": [
    "# Section 1: Define Two-Tower Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56169c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoTowerModel(nn.Module):\n",
    "    def __init__(self, user_dim, content_dim, embedding_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.user_tower = nn.Sequential(\n",
    "            nn.Linear(user_dim, embedding_dim),\n",
    "            nn.BatchNorm1d(embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            nn.BatchNorm1d(embedding_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.content_tower = nn.Sequential(\n",
    "            nn.Linear(content_dim, embedding_dim),\n",
    "            nn.BatchNorm1d(embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            nn.BatchNorm1d(embedding_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.output_layer = nn.Linear(embedding_dim * 2, 1)  # regression\n",
    "\n",
    "    def forward(self, u, c):\n",
    "        u_vec = self.user_tower(u)\n",
    "        c_vec = self.content_tower(c)\n",
    "        combined = torch.cat([u_vec, c_vec], dim=1)\n",
    "        out = self.output_layer(combined)\n",
    "        return out.squeeze(-1), u_vec, c_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac962d97",
   "metadata": {},
   "source": [
    "# Section 2: Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591025ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "def to_tensor(x):\n",
    "    if torch.is_tensor(x):\n",
    "        return x.float()\n",
    "    elif hasattr(x, \"values\"):\n",
    "        return torch.tensor(x.values, dtype=torch.float32)\n",
    "    else:\n",
    "        return torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "def create_dataloaders(tX_user, tX_content, ty, vX_user, vX_content, vy, batch_size):\n",
    "    train_loader = DataLoader(TensorDataset(tX_user, tX_content, ty), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(vX_user, vX_content, vy), batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ba1a35",
   "metadata": {},
   "source": [
    "# Section 3: Define Objective Function for Optuna HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0e92e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-19 23:08:30,952] A new study created in memory with name: no-name-5d505082-1f8c-434c-88c8-d7d43b986fef\n",
      "2025/10/19 23:09:49 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\u001b[31m2025/10/19 23:09:55 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "[I 2025-10-19 23:09:55,640] Trial 0 finished with value: 0.002391652960795909 and parameters: {'embedding_dim': 96, 'dropout': 0.15486829135126262, 'lr': 0.00010723596714808144, 'batch_size': 2048}. Best is trial 0 with value: 0.002391652960795909.\n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x103a25cf0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/guptayas/.pyenv/versions/langchain-env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "def objective(trial, user_dim, content_dim, tX_user, tX_content, ty, vX_user, vX_content, vy):\n",
    "    # Sample hyperparameters\n",
    "    embedding_dim = trial.suggest_categorical(\"embedding_dim\", MODEL_CONFIG[\"hpo_params\"][\"embedding_dim\"])\n",
    "    dropout = trial.suggest_float(\"dropout\", *MODEL_CONFIG[\"hpo_params\"][\"dropout_range\"])\n",
    "    lr = trial.suggest_float(\"lr\", *MODEL_CONFIG[\"hpo_params\"][\"lr_range\"], log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", MODEL_CONFIG[\"hpo_params\"][\"batch_size\"])\n",
    "\n",
    "    # Dataloaders\n",
    "    train_loader, val_loader = create_dataloaders(tX_user, tX_content, ty, vX_user, vX_content, vy, batch_size)\n",
    "\n",
    "    # MLflow tracking\n",
    "    os.makedirs(MODEL_CONFIG[\"mlruns_dir\"], exist_ok=True)\n",
    "    mlflow.set_tracking_uri(f\"file:{MODEL_CONFIG['mlruns_dir']}\")\n",
    "    mlflow.set_experiment(MODEL_CONFIG[\"experiment_name\"])\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"optuna_trial_{trial.number}\"):\n",
    "        mlflow.log_params({\n",
    "            \"embedding_dim\": embedding_dim,\n",
    "            \"dropout\": dropout,\n",
    "            \"lr\": lr,\n",
    "            \"batch_size\": batch_size\n",
    "        })\n",
    "\n",
    "        model = TwoTowerModel(user_dim, content_dim, embedding_dim, dropout)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(MODEL_CONFIG[\"num_epochs\"]):\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for u, c, yb in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                preds, _, _ = model(u, c)\n",
    "                loss = criterion(preds, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for u, c, yb in val_loader:\n",
    "                    preds, _, _ = model(u, c)\n",
    "                    val_loss += criterion(preds, yb).item()\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "            # Log metrics\n",
    "            mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
    "            mlflow.log_metric(\"val_loss\", avg_val_loss, step=epoch)\n",
    "\n",
    "        # Log model\n",
    "        mlflow.pytorch.log_model(model, \"model\")\n",
    "\n",
    "        trial.set_user_attr(\"run_id\", mlflow.active_run().info.run_id)\n",
    "        return avg_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149135d0",
   "metadata": {},
   "source": [
    "# Section 4: Run HPO Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e824a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess data\n",
    "    data = load_data(MODEL_CONFIG[\"data_path\"])\n",
    "    tX_user, tX_content, ty = to_tensor(data[\"tX_user\"]), to_tensor(data[\"tX_content\"]), to_tensor(data[\"ty\"])\n",
    "    vX_user, vX_content, vy = to_tensor(data[\"vX_user\"]), to_tensor(data[\"vX_content\"]), to_tensor(data[\"vy\"])\n",
    "\n",
    "    user_dim, content_dim = tX_user.shape[1], tX_content.shape[1]\n",
    "\n",
    "    # Run Optuna HPO\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(lambda trial: objective(trial, user_dim, content_dim, tX_user, tX_content, ty,\n",
    "                                           vX_user, vX_content, vy),\n",
    "                   n_trials=MODEL_CONFIG[\"num_trials\"])\n",
    "\n",
    "    # Print best trial\n",
    "    best_trial = study.best_trial\n",
    "    print(f\"Best Validation Loss: {best_trial.value:.6f}\")\n",
    "    print(\"Best Hyperparameters:\")\n",
    "    for k, v in best_trial.params.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    print(f\"Best MLflow run_id: {best_trial.user_attrs['run_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06782de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/guptayas/.pyenv/versions/3.10.13/envs/langchain-env/lib/python3.10/site-packages/mlflow/gateway/config.py:454: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  class Route(ConfigModel):\n",
      "[MLflow] Security middleware enabled with default settings (localhost-only). To allow connections from other hosts, use --host 0.0.0.0 and configure --allowed-hosts and --cors-allowed-origins.\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://127.0.0.1:5000\u001b[0m (Press CTRL+C to quit)\n",
      "\u001b[32mINFO\u001b[0m:     Started parent process [\u001b[36m\u001b[1m6683\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m6688\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m6685\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m6686\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m6687\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:51934 - \"\u001b[1mGET / HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:51934 - \"\u001b[1mGET /static-files/manifest.json HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:51934 - \"\u001b[1mGET /ajax-api/2.0/mlflow/experiments/search?max_results=5&order_by=last_update_time+DESC HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:51934 - \"\u001b[1mGET /ajax-api/2.0/mlflow/experiments/search?max_results=25&order_by=last_update_time+DESC HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:51934 - \"\u001b[1mPOST /graphql HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:51934 - \"\u001b[1mGET /ajax-api/2.0/mlflow/traces?experiment_ids=764276176896151570&order_by=timestamp_ms%20DESC&max_results=1&filter= HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:51938 - \"\u001b[1mPOST /ajax-api/2.0/mlflow/experiments/search-datasets HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:51934 - \"\u001b[1mGET /ajax-api/2.0/mlflow/gateway-proxy?gateway_path=api%2F2.0%2Fendpoints%2F HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:51938 - \"\u001b[1mPOST /ajax-api/2.0/mlflow/logged-models/search HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:51937 - \"\u001b[1mPOST /ajax-api/2.0/mlflow/runs/search HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:51939 - \"\u001b[1mPOST /ajax-api/2.0/mlflow/runs/search HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:51934 - \"\u001b[1mGET /ajax-api/2.0/mlflow/gateway-proxy?gateway_path=api%2F2.0%2Fendpoints%2F HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:51938 - \"\u001b[1mPOST /ajax-api/2.0/mlflow/logged-models/search HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:51937 - \"\u001b[1mPOST /ajax-api/2.0/mlflow/runs/search HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:51942 - \"\u001b[1mGET /ajax-api/2.0/mlflow/model-versions/search?filter=run_id%3D%277aba512c9af042eb9c5a2b69808ab497%27 HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:51941 - \"\u001b[1mPOST /graphql HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:51942 - \"\u001b[1mGET /ajax-api/2.0/mlflow/model-versions/search?filter=tags.%60mlflow.prompt.is_prompt%60+%3D+%27true%27+AND+tags.%60mlflow.prompt.run_ids%60+ILIKE+%22%257aba512c9af042eb9c5a2b69808ab497%25%22 HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:51941 - \"\u001b[1mPOST /ajax-api/2.0/mlflow/logged-models/search HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:51942 - \"\u001b[1mGET /ajax-api/2.0/mlflow/metrics/get-history-bulk-interval?run_ids=7aba512c9af042eb9c5a2b69808ab497&metric_key=train_loss&max_results=320 HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:51941 - \"\u001b[1mGET /ajax-api/2.0/mlflow/metrics/get-history-bulk-interval?run_ids=7aba512c9af042eb9c5a2b69808ab497&metric_key=val_loss&max_results=320 HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:51946 - \"\u001b[1mGET /ajax-api/2.0/mlflow/metrics/get-history-bulk-interval?run_ids=7aba512c9af042eb9c5a2b69808ab497&metric_key=train_loss&max_results=320 HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:51946 - \"\u001b[1mGET /ajax-api/2.0/mlflow/metrics/get-history-bulk-interval?run_ids=7aba512c9af042eb9c5a2b69808ab497&metric_key=val_loss&max_results=320 HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "\u001b[32mINFO\u001b[0m:     127.0.0.1:51947 - \"\u001b[1mGET /ajax-api/2.0/mlflow/model-versions/search?filter=tags.%60mlflow.prompt.is_prompt%60+%3D+%27true%27+AND+tags.%60mlflow.prompt.run_ids%60+ILIKE+%22%257aba512c9af042eb9c5a2b69808ab497%25%22 HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
      "^C\n",
      "\n",
      "Aborted!\n",
      "\u001b[32mINFO\u001b[0m:     Shutting down\n",
      "\u001b[32mINFO\u001b[0m:     Received SIGINT, exiting.\n",
      "\u001b[32mINFO\u001b[0m:     Received SIGINT, exiting.\n",
      "\u001b[32mINFO\u001b[0m:     Terminated child process [6685]\n",
      "\u001b[32mINFO\u001b[0m:     Terminated child process [6686]\n",
      "\u001b[32mINFO\u001b[0m:     Terminated child process [6687]\n",
      "\u001b[32mINFO\u001b[0m:     Terminated child process [6688]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for child process [6685]\n",
      "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m6687\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m6685\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m6688\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for child process [6686]\n",
      "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m6686\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for child process [6687]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for child process [6688]\n",
      "\u001b[32mINFO\u001b[0m:     Stopping parent process [\u001b[36m\u001b[1m6683\u001b[0m]\n"
     ]
    }
   ],
   "source": [
    "! mlflow ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3843237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ins_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
